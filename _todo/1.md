### 2. Статистические метрики (числовые оценки "кривизны")

#### Энтропия (Entropy)
Это ключевая метрика из теории информации, которая измеряет **уровень неопределенности** или "хаоса" в распределении.

*   **Максимальная энтропия:** У равномерного распределения (все вероятности равны). Чем больше у вас категорий, тем выше будет максимальное значение энтропии.
*   **Минимальная энтропия (равна 0):** У полностью определенного распределения, где одна вероятность равна 1, а остальные — 0.

**Чем ниже энтропия, тем более "кривое" и концентрированное у вас распределение.**

**Пример кода:**
Как видите, у "кривого" распределения энтропия низкая (0.27), а у равномерного — максимальная (1.0).

#### Простые описательные статистики
Можно посмотреть на сами значения вероятностей.

*   **Максимальная вероятность:** `max(probabilities)`. Если она близка к 1, распределение очень концентрированное.
*   **Стандартное отклонение:** `std(probabilities)`.
    *   **Высокое** стандартное отклонение означает, что значения вероятностей сильно отличаются друг от друга (например, [0.9, 0.1, 0, 0]). Это признак "кривого" распределения.
    *   **Низкое** стандартное отклонение (близкое к 0) означает, что все вероятности очень близки друг к другу, то есть к равномерному распределению.

### 3. Сравнение с эталонным распределением

#### Тест Хи-квадрат (Chi-squared Goodness of Fit Test)
Этот тест позволяет статистически проверить, **значимо ли отличается ваше распределение от некоторого теоретического** (например, от равномерного).

*   **Нулевая гипотеза (H0):** Ваше распределение не отличается от равномерного.
*   **Что смотреть:** На **p-value**.
    *   Если **p-value < 0.05** (или другого выбранного вами уровня значимости), вы отвергаете нулевую гипотезу. Это означает, что ваше распределение **статистически значимо отличается от равномерного**, то есть оно "кривое".
    *   Если **p-value > 0.05**, у вас нет оснований считать, что распределение не является равномерным.

**Важно:** Тест работает с частотами (количеством наблюдений), а не с вероятностями. Чтобы его применить, нужно умножить ваши вероятности на общее число наблюдений (N). Если у вас просто столбец вероятностей без контекста, можно взять N=100 для примера.

**Пример кода:**
Результат `p-value` для концентрированного распределения чрезвычайно мал, что подтверждает его "кривизну".

### Итог: Что выбрать?

| Метод | Что показывает | Когда использовать |
| :--- | :--- | :--- |
| **Столбчатая диаграмма** | Визуальную форму распределения | **Всегда.** Это лучший первый шаг. |
| **Энтропия** | Числовую меру неопределенности/концентрации | Когда нужно одно число для сравнения "кривизны" разных распределений. |
| **Макс. вероятность / Стандартное отклонение** | Простые индикаторы концентрации и разброса | Для быстрой и грубой оценки. |
| **Тест Хи-квадрат** | Статистическую значимость отличия от равномерного | Когда нужна формальная статистическая проверка. |

Вместо проверки на "нормальность", для анализа столбца вероятностей исследуют их **концентрацию**, **неопределенность** и **отличие от равномерного распределения**.

а как проверить что вектор, представляющий собой вероятности - распределен симметричным холником и как это называется

__|__
_|||_
|||||